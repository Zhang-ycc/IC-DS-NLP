{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Representation in Biomedical Domain\n",
    "\n",
    "Before you start, please make sure you have read this notebook. You are encouraged to follow the recommendations but you are also free to develop your own solution from scratch. \n",
    "\n",
    "## Marking Scheme\n",
    "\n",
    "- Biomedical imaging project: 40%\n",
    "    - 20%: accuracy of the final model on the test set\n",
    "    - 20%: rationale of model design and final report\n",
    "- Natural language processing project: 40%\n",
    "    - 30%: completeness of the project\n",
    "    - 10%: final report\n",
    "- Presentation skills and team work: 20%\n",
    "\n",
    "\n",
    "This project forms 40\\% of the total score for summer/winter school. The marking scheme of each part of this project is provided below with a cap of 100\\%.\n",
    "\n",
    "You are allowed to use open source libraries as long as the libraries are properly cited in the code and final report. The usage of third-party code without proper reference will be treated as plagiarism, which will not be tolerated.\n",
    "\n",
    "You are encouraged to develop the algorithms by yourselves (without using third-party code as much as possible). We will factor such effort into the marking process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Prerequisites \n",
    "\n",
    "Recommended environment\n",
    "\n",
    "- Python 3.7 or newer\n",
    "- Free disk space: 100GB\n",
    "\n",
    "Download the data\n",
    "\n",
    "```sh\n",
    "# navigate to the data folder\n",
    "cd data\n",
    "\n",
    "# download the data file\n",
    "# which is also available at https://www.semanticscholar.org/cord19/download\n",
    "wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2021-07-26/document_parses.tar.gz\n",
    "\n",
    "# decompress the file which may take several minutes\n",
    "tar -xf document_parses.tar.gz\n",
    "\n",
    "# which creates a folder named document_parses\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 (20%): Parse the Data\n",
    "\n",
    "The JSON files are located in two sub-folders in `document_parses`. You will need to scan all JSON files and extract text (i.e. `string`) from relevant fields (e.g. body text, abstract, titles).\n",
    "\n",
    "You are encouraged to extract full article text from body text if possible. If the hardware resource is limited, you can extract from abstract or titles as alternatives. \n",
    "\n",
    "Note: The number of JSON files is around 425k so it may take more than 10 minutes to parse all documents.\n",
    "\n",
    "For more information about the dataset: https://www.semanticscholar.org/cord19/download\n",
    "\n",
    "Recommended output:\n",
    "\n",
    "- A list of text (`string`) extracted from JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing papers: 100%|██████████| 425257/425257 [04:47<00:00, 1479.25paper/s]\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "# TODO: add your solution\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def parse_data() -> list[str]:\n",
    "    \"\"\" Scans all JSON files in folder `data` and extracts full text from them.\n",
    "\n",
    "    Returns:\n",
    "       list[str]: A list containing full article text of all papers.\n",
    "    \"\"\"    \n",
    "\n",
    "    res = []\n",
    "\n",
    "    doc_dir = os.path.join(\"data\", \"document_parses\")\n",
    "    pdf_dir = os.path.join(doc_dir, \"pdf_json\")\n",
    "    pmc_dir = os.path.join(doc_dir, \"pmc_json\")\n",
    "    file_paths = [os.path.join(pdf_dir, filename) for filename in os.listdir(pdf_dir)]\n",
    "    file_paths.extend([os.path.join(pmc_dir, filename) for filename in os.listdir(pmc_dir)])\n",
    "    \n",
    "    with tqdm(file_paths, desc=\"Parsing papers\", unit=\"paper\") as pbar:\n",
    "        for file_path in pbar:\n",
    "            with open(file_path, encoding='utf-8') as f:\n",
    "                paper = json.load(f)\n",
    "                body_text = paper['body_text']\n",
    "                res.append('\\n'.join(para['text'] for para in body_text))\n",
    "\n",
    "    return res\n",
    "\n",
    "full_texts = parse_data()\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425257\n",
      "According to current live statistics at the time of editing this letter, Russia has been the third country in the world to be affected by COVID-19 with both new cases and death rates rising. It remains in a position of advantage due to the later onset of the viral spread within the country since the worldwide disease outbreak.\n",
      "The first step in \"fighting\" the epidemic was nationwide lock down on March 30 th , 2020.\n",
      "Most of the multidisciplinary hospitals have been repurposed as dedicated COVID-19 centres, so the surgeons started working as infectious disease specialists. Such a reallocation of health care capacity results in the effective management of this epidemiological problem 1 . The staff has undergone on-line 36-hour training course to become qualified in coronavirus infection treatment.\n",
      "The surgeons of COVID-19 dedicated hospitals do rarely practice surgery. When ICU patients need mechanical ventilation, percutaneous tracheostomy under endoscopic control is mostly performed, as it decreases the aerosol formation, viral load on staff and complications, associated with an endotracheal tube in comparison with surgical tracheostomy 2 . However, it is still associated with the risk of aerosol formation, so different approaches should be considered for a long-time perspective 3 .\n",
      "The majority of the studies dedicated to colorectal diseases are temporarily paused. The teaching and training are mostly translated via online platforms, which has excluded the opportunity to get clinical experience in surgery 4 .\n",
      "The approach to patient routing has changed significantly. If one is not diagnosed with COVID-19 CT scan and laboratory testing are provided immediately. The patients should be admitted to the surgical department, where treatment is provided only to those COVID-19 negative.\n",
      "The patient isolated for more than 2 weeks and COVID-19 negative as a result of 2 subsequent tests is admitted to the surgical department with an option to readmission to the infectious department and can be treated by surgical staff, which does not work with COVID-19 positive patients.\n",
      "The patient, diagnosed with coronavirus infection and treated at home is admitted to COVID-19 dedicated multidisciplinary hospital, where surgical care is provided. Those treated in infectious diseases hospital or COVID-19 dedicated centre are managed by the surgical team present.\n",
      "Surgery has become highly elective, being mostly available for high-risk patients with emergencies, malignancies, cardiovascular pathologies or infections. Preoperative testing in surgical patients with respiratory symptoms and history of travelling or contacting with COVID-19 positive people and postoperative recovery in the operating unit seem to be highly effective measures 5 . A lot of rearrangements are performed locally regarding personal protective equipment, the organization of scrubbing, donning and doffing, and dedicated changing areas. Moreover, observational departments are organized in surgical hospitals for patient allocation before coronavirus infection status is defined 6 .\n",
      "Surgery for benign disorders, precancerous lesions, and reconstructive procedures are currently postponed. Regarding colorectal cancer, surgical treatment may be postponed, if it is a non-obstructing disease 7 .\n",
      "Laparoscopic surgery and diathermy are limited as well. The importance of special operating theatre for COVID-19 patients with negative pressure ventilation, unidirectional laminar flow, as well as the use of smoke evacuation systems during surgery are taken into account 8 .\n",
      "Such an electiveness of surgery is concerning, as it might cause a worldwide healthcare catastrophe in the post-pandemic era 5 . More efforts should be taken to expand the amount and types of surgical procedures performed.\n",
      "Due to the early preventive and corrective actions we have already reached the plateau in new cases curve, counting for up to 8 984 cases identified at the time of writing this paper (June 7 th , 2020), with a mortality rate of 1⋅5075%. These statistical outcomes are resulted by a 68-day lockdown, admission regime, and healthcare rearrangement. Thus, the multistep restriction lifting has already started to consistently recover in both social and economic aspects. \n"
     ]
    }
   ],
   "source": [
    "print(len(full_texts))\n",
    "print(full_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 (30%): Tokenization\n",
    "\n",
    "Traverse the extracted text and segment the text into words (or tokens).\n",
    "\n",
    "The following tracks can be developed in independentely. You are encouraged to divide the workload to each team member.\n",
    "\n",
    "Recommended output:\n",
    "\n",
    "- Tokenizer(s) that is able to tokenize any input text.\n",
    "\n",
    "Note: Because of the computation complexity of tokenizers, it may take hours/days to process all documents. Which tokenizer is more efficient? Any idea to speedup?\n",
    "\n",
    "### Track 2.1 (10%): Use split()\n",
    "\n",
    "Use the standard `split()` by Python.\n",
    "\n",
    "### Track 2.2 (10%): Use NLTK or SciSpaCy\n",
    "\n",
    "NLTK tokenizer: https://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "SciSpaCy: https://github.com/allenai/scispacy\n",
    "\n",
    "Note: You may need to install NLTK and SpaCy so please refer to their websites for installation instructions.\n",
    "\n",
    "### Track 2.3 (10%): Use Byte-Pair Encoding (BPE)\n",
    "\n",
    "Byte-Pair Encoding (BPE): https://huggingface.co/transformers/tokenizer_summary.html\n",
    "\n",
    "Note: You may need to install Huggingface's transformers so please refer to its website for installation instructions.\n",
    "\n",
    "### Track 2.4 (Bonus +5%): Build new Byte-Pair Encoding (BPE)\n",
    "\n",
    "This track may be dependent on track 2.3.\n",
    "\n",
    "The above pre-built tokenization methods may not be suitable for biomedical domain as the words/tokens (e.g. diseases, sympotoms, chemicals, medications, phenotypes, genotypes etc.) can be very different from the words/tokens commonly used in daily life. Can you build and train a new BPE model for biomedical domain in particular?\n",
    "\n",
    "### Open Question (Optional):\n",
    "\n",
    "- What are the pros and cons of the above tokenizers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenize:  12%|█▏        | 49023/425257 [03:54<41:11, 152.21text/s]  "
     ]
    }
   ],
   "source": [
    "###################\n",
    "# TODO: add your solution\n",
    "\n",
    "# Track 2.1\n",
    "\n",
    "import re\n",
    "from gensim import corpora\n",
    "\n",
    "def tokenizer_1():\n",
    "    res = []\n",
    "    for text in tqdm(full_texts, desc=\"tokenize\", unit=\"text\"):\n",
    "        res.append([word for word in re.split(r'\\W+', text.lower()) if word != ''])\n",
    "    return res\n",
    "\n",
    "texts = tokenizer_1()\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save('token.dict')  # store the dictionary, for future reference\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(dictionary.doc2idx(re.split(r'\\W+', full_texts[0].lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: token2id id2token\n",
    "\"\"\"\n",
    "\n",
    "class TokenizerSplit(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.dictionary = None\n",
    "\n",
    "    def tokenize(self, in_texts):\n",
    "        res = []\n",
    "        for text in tqdm(in_texts, desc=\"tokenize\", unit=\"text\"):\n",
    "            res.append([word for word in re.split(r'\\W+', text.lower()) if word != ''])\n",
    "        self.dictionary = corpora.Dictionary(res)\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self.dictionary.doc2idx([word for word in re.split(r'\\W+', text.lower()) if word != ''])\n",
    "\n",
    "    def decode(self, cor):\n",
    "        text = ''\n",
    "        for token_id in cor:\n",
    "            text += dictionary.id2token(cor)\n",
    "        return text\n",
    "\n",
    "    def getTokenId(self, word):\n",
    "        return dictionary.token2id(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\KWZhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Track 2.2\n",
    "\n",
    "\"\"\"\"\n",
    "install nltk, run for one time\n",
    "\"\"\"\n",
    "\n",
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenize:   1%|▏         | 5848/425257 [03:36<4:07:48, 28.21text/s] "
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenizer_2():\n",
    "    res = []\n",
    "    for text in tqdm(full_texts, desc=\"tokenize\", unit=\"text\"):\n",
    "        res.append([word for word in word_tokenize(text.lower()) if word != ''])\n",
    "    return res\n",
    "\n",
    "texts_2 = tokenizer_2()\n",
    "dictionary_2 = corpora.Dictionary(texts_2)\n",
    "dictionary_2.save('token_2.dict')  # store the dictionary, for future reference\n",
    "print(dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class TokenizerNLTK(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.dictionary = None\n",
    "\n",
    "    def tokenize(self, in_texts):\n",
    "        res = []\n",
    "        for text in tqdm(in_texts, desc=\"tokenize\", unit=\"text\"):\n",
    "            res.append([word for word in word_tokenize(text.lower()) if word != ''])\n",
    "        self.dictionary = corpora.Dictionary(res)\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self.dictionary.doc2idx([word for word in re.split(r'\\W+', text.lower()) if word != ''])\n",
    "\n",
    "    def decode(self, cor):\n",
    "        text = ''\n",
    "        for token_id in cor:\n",
    "            text += dictionary.id2token(cor)\n",
    "        return text\n",
    "\n",
    "    def getTokenId(self, word):\n",
    "        return dictionary.token2id(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# Track 2.3\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from itertools import chain\n",
    "import re\n",
    "from nltk import sent_tokenize\n",
    "from typing import List, Dict, Iterable, Union, Optional\n",
    "\n",
    "\n",
    "class BPETokenizer:\n",
    "    \n",
    "    def __init__(self, from_file:bool=False, tokenizer_path:Optional[str]=None):\n",
    "        \"\"\" Initializes a BPE tokenizer or loads an existing tokenizer from file.\n",
    "\n",
    "        Args:\n",
    "            from_file (bool): Whether to load an existing tokenizer from file. Defaults to `False`.\n",
    "            tokenizer_path (Optional[str]): The path of the tokenizer (if `from_file` is `True`).\n",
    "        \"\"\"        \n",
    "        if from_file:  # load a trained tokenizer\n",
    "            assert tokenizer_path\n",
    "            self.tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "        else:  # initialize a tokenizer\n",
    "            self.tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "            self.tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "\n",
    "    def normalize(self, text: Union[str, List[str]]) -> Iterable[str]:\n",
    "        \"\"\" Normalizes the text, that is, converts uppercase to lowercase and removes punctuations.\n",
    "\n",
    "        Args:\n",
    "            text (Union[str, List[str]]): Text to normalize.\n",
    "\n",
    "        Returns:\n",
    "            Iterable[str]: Iterator of normalized sentences.\n",
    "        \"\"\"        \n",
    "        if isinstance(text, str):\n",
    "            text = [text]\n",
    "        # separate sentences\n",
    "        text = map(lambda doc: sent_tokenize(doc.lower()), text)  # iterator of list of sentences\n",
    "        text = chain.from_iterable(text)  # iterator of sentences\n",
    "        # remove punctuations\n",
    "        punc_pat = re.compile(R\"[^\\w\\s'-]+\")  # punctuations that should be removed\n",
    "        return map(lambda doc: punc_pat.sub(\"\", doc), text)\n",
    "\n",
    "\n",
    "    def train(self, text:List[str], vocab_size:int=10000, save_path:Optional[str]=None):\n",
    "        \"\"\" Trains on corpus.\n",
    "\n",
    "        Args:\n",
    "            text (List[str]): A list of documents.\n",
    "            vocab_size (int): The desired vocabulary size. Defaults to `10000`.\n",
    "            tokenizer_path (Optional[str]): The path to store the trained tokenizer in.\n",
    "        \"\"\"        \n",
    "        trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], vocab_size=vocab_size)\n",
    "        text = self.normalize(text)\n",
    "        self.tokenizer.train_from_iterator(text, trainer)\n",
    "        if save_path:\n",
    "            self.tokenizer.save(save_path)\n",
    "\n",
    "    \n",
    "    def save(self, save_path: str):\n",
    "        \"\"\" Saves the tokenizer to file.\n",
    "\n",
    "        Args:\n",
    "            tokenizer_path (str): The path to store the trained tokenizer in.\n",
    "        \"\"\"        \n",
    "        self.tokenizer.save(save_path)\n",
    "\n",
    "\n",
    "    def encode(self, text: str) -> Dict[str, List[Union[str, int]]]:\n",
    "        \"\"\" Encodes a string.\n",
    "\n",
    "        Args:\n",
    "            text (str): A string to encode.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, List[Union[str, int]]]: Output of encoding, including \"tokens\" and \"ids\".\n",
    "        \"\"\"        \n",
    "        text = self.normalize(text)\n",
    "        tokens = ['[CLS]']\n",
    "        ids = [self.token_to_id('[CLS]')]\n",
    "        for stc in text:\n",
    "            enc = self.tokenizer.encode(stc)\n",
    "            tokens.extend(enc.tokens)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            ids.extend(enc.ids)\n",
    "            ids.append(self.token_to_id('[SEP]'))\n",
    "        return {\"tokens\": tokens, \"ids\": ids}\n",
    "\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        \"\"\" Decodes a sequence of ids.\n",
    "\n",
    "        Args:\n",
    "            ids (List[int]): A list of ids of tokens.\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded string.\n",
    "        \"\"\"        \n",
    "        return \" \".join([self.id_to_token(id) for id in ids])\n",
    "\n",
    "\n",
    "    def add_tokens(self, tokens: List[str]):\n",
    "        \"\"\" Adds given tokens to vocabulary.\n",
    "\n",
    "        Args:\n",
    "            tokens (List[str]): Tokens to add.\n",
    "        \"\"\"        \n",
    "        self.tokenizer.add_tokens([tkn.lower() for tkn in tokens])\n",
    "\n",
    "    \n",
    "    def token_to_id(self, token: str) -> int:\n",
    "        return self.tokenizer.token_to_id(token)\n",
    "\n",
    "    def id_to_token(self, id: int) -> str:\n",
    "        return self.tokenizer.id_to_token(id)\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return self.tokenizer.get_vocab()\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return self.tokenizer.get_vocab_size()\n",
    "\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a BPE tokenizer\n",
    "\n",
    "tokenizer = BPETokenizer()\n",
    "tokenizer.train(full_texts, vocab_size=10000, save_path=\"BPETokenizer10000.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n",
      "college\n",
      "9999\n",
      "{'tokens': ['[CLS]', 'how', 'are', 'you', '[SEP]', 'i', \"'\", 'm', 'fine', 'than', 'k', 'you', 'and', 'you', '[SEP]', 'i', \"'\", 'm', 'fine', 'too', '[SEP]'], 'ids': [1, 5805, 4877, 7095, 2, 26, 5, 30, 11336, 5161, 28, 7095, 4830, 7095, 2, 26, 5, 30, 11336, 8174, 2]}\n",
      "[CLS] how are you [SEP] i ' m fine than k you and you [SEP] i ' m fine too [SEP]\n",
      "['[CLS]', 'diseases', '[SEP]']\n",
      "['[CLS]', 'symptoms', '[SEP]']\n",
      "['[CLS]', 'chemicals', '[SEP]']\n",
      "['[CLS]', 'medications', '[SEP]']\n",
      "['[CLS]', 'phenotypes', '[SEP]']\n",
      "['[CLS]', 'genotypes', '[SEP]']\n",
      "['[CLS]', 'covid', '-', '19', '[SEP]']\n",
      "['[CLS]', 'i', 'have', '300', '00', 'ap', 'ples', '[SEP]']\n",
      "15002\n",
      "['[CLS]', 'phenotypes', '[SEP]']\n",
      "['[CLS]', 'genotypes', '[SEP]']\n",
      "['[CLS]', 'covid-19', '[SEP]']\n",
      "['[CLS]', 'i', 'have', '30000', 'ap', 'ples', '[SEP]']\n",
      "['[CLS]', 'the', 'main', 'symptoms', 'of', 'covid-19', 'are', 'fever', 'and', 'cough', '[SEP]']\n",
      "['[CLS]', 'according', 'to', 'current', 'live', 'statistics', 'at', 'the', 'time', 'of', 'editing', 'this', 'letter', 'russia', 'has', 'been', 'the', 'third', 'country', 'in', 'the', 'world', 'to', 'be', 'affected', 'by', 'covid-19', 'with', 'both', 'new', 'cases', 'and', 'death', 'rates', 'rising', '[SEP]', 'it', 'remains', 'in', 'a', 'position', 'of', 'advantage', 'due', 'to', 'the', 'later', 'onset', 'of', 'the', 'viral', 'spread', 'within', 'the', 'country', 'since', 'the', 'worldwide', 'disease', 'outbreak', '[SEP]', 'the', 'first', 'step', 'in', 'f', 'ighting', 'the', 'epidemic', 'was', 'nationwide', 'lock', 'down', 'on', 'march', '30', 'th', '2020', 'most', 'of', 'the', 'multidisciplinary', 'hospitals', 'have', 'been', 'repur', 'posed', 'as', 'dedicated', 'covid-19', 'centres', 'so', 'the', 'surgeons', 'started', 'working', 'as', 'infectious', 'disease', 'specialists', '[SEP]', 'such', 'a', 'real', 'location', 'of', 'health', 'care', 'capacity', 'results', 'in', 'the', 'effective', 'management', 'of', 'this', 'epidemiological', 'problem', '1', '[SEP]', 'the', 'staff', 'has', 'under', 'gone', 'on', '-', 'line', '36', '-', 'hour', 'training', 'course', 'to', 'become', 'qual', 'ified', 'in', 'coronavirus', 'infection', 'treatment', '[SEP]', 'the', 'surgeons', 'of', 'covid-19', 'dedicated', 'hospitals', 'do', 'rarely', 'practice', 'surgery', '[SEP]', 'when', 'icu', 'patients', 'need', 'mechanical', 'ventilation', 'per', 'cutaneous', 'trache', 'ostomy', 'under', 'endosco', 'pic', 'control', 'is', 'mostly', 'performed', 'as', 'it', 'decreases', 'the', 'aerosol', 'formation', 'viral', 'load', 'on', 'staff', 'and', 'complications', 'associated', 'with', 'an', 'endo', 'tracheal', 'tube', 'in', 'comparison', 'with', 'surgical', 'trache', 'ostomy', '2', '[SEP]', 'however', 'it', 'is', 'still', 'associated', 'with', 'the', 'risk', 'of', 'aerosol', 'formation', 'so', 'different', 'approaches', 'should', 'be', 'considered', 'for', 'a', 'long', '-', 'time', 'perspective', '3', '[SEP]', 'the', 'majority', 'of', 'the', 'studies', 'dedicated', 'to', 'colo', 'rectal', 'diseases', 'are', 'tempor', 'arily', 'pa', 'used', '[SEP]', 'the', 'teaching', 'and', 'training', 'are', 'mostly', 'translated', 'via', 'online', 'platforms', 'which', 'has', 'excluded', 'the', 'opportunity', 'to', 'get', 'clinical', 'experience', 'in', 'surgery', '4', '[SEP]', 'the', 'approach', 'to', 'patient', 'rout', 'ing', 'has', 'changed', 'significantly', '[SEP]', 'if', 'one', 'is', 'not', 'diagnosed', 'with', 'covid-19', 'ct', 'scan', 'and', 'laboratory', 'testing', 'are', 'provided', 'immediately', '[SEP]', 'the', 'patients', 'should', 'be', 'admitted', 'to', 'the', 'surgical', 'department', 'where', 'treatment', 'is', 'provided', 'only', 'to', 'those', 'covid-19', 'negative', '[SEP]', 'the', 'patient', 'isolated', 'for', 'more', 'than', '2', 'weeks', 'and', 'covid-19', 'negative', 'as', 'a', 'result', 'of', '2', 'subsequent', 'tests', 'is', 'admitted', 'to', 'the', 'surgical', 'department', 'with', 'an', 'option', 'to', 'read', 'mission', 'to', 'the', 'infectious', 'department', 'and', 'can', 'be', 'treated', 'by', 'surgical', 'staff', 'which', 'does', 'not', 'work', 'with', 'covid-19', 'positive', 'patients', '[SEP]', 'the', 'patient', 'diagnosed', 'with', 'coronavirus', 'infection', 'and', 'treated', 'at', 'home', 'is', 'admitted', 'to', 'covid-19', 'dedicated', 'multidisciplinary', 'hospital', 'where', 'surgical', 'care', 'is', 'provided', '[SEP]', 'those', 'treated', 'in', 'infectious', 'diseases', 'hospital', 'or', 'covid-19', 'dedicated', 'centre', 'are', 'managed', 'by', 'the', 'surgical', 'team', 'present', '[SEP]', 'surgery', 'has', 'become', 'highly', 'elective', 'being', 'mostly', 'available', 'for', 'high', '-', 'risk', 'patients', 'with', 'emergencies', 'malignancies', 'cardiovascular', 'pathologies', 'or', 'infections', '[SEP]', 'preoperative', 'testing', 'in', 'surgical', 'patients', 'with', 'respiratory', 'symptoms', 'and', 'history', 'of', 'trave', 'lling', 'or', 'contact', 'ing', 'with', 'covid-19', 'positive', 'people', 'and', 'postoperative', 'recovery', 'in', 'the', 'operating', 'unit', 'seem', 'to', 'be', 'highly', 'effective', 'measures', '5', '[SEP]', 'a', 'lot', 'of', 're', 'arrangements', 'are', 'performed', 'locally', 'regarding', 'personal', 'protective', 'equipment', 'the', 'organization', 'of', 'scr', 'ub', 'bing', 'don', 'ning', 'and', 'd', 'off', 'ing', 'and', 'dedicated', 'changing', 'areas', '[SEP]', 'moreover', 'observational', 'departments', 'are', 'organized', 'in', 'surgical', 'hospitals', 'for', 'patient', 'allocation', 'before', 'coronavirus', 'infection', 'status', 'is', 'defined', '6', '[SEP]', 'surgery', 'for', 'ben', 'ign', 'disorders', 'pre', 'cancer', 'ous', 'lesions', 'and', 're', 'constru', 'ctive', 'procedures', 'are', 'currently', 'post', 'pon', 'ed', '[SEP]', 'regarding', 'colo', 'rectal', 'cancer', 'surgical', 'treatment', 'may', 'be', 'post', 'pon', 'ed', 'if', 'it', 'is', 'a', 'non', '-', 'ob', 'struct', 'ing', 'disease', '7', '[SEP]', 'lapar', 'os', 'co', 'pic', 'surgery', 'and', 'dia', 'ther', 'my', 'are', 'limited', 'as', 'well', '[SEP]', 'the', 'importance', 'of', 'special', 'operating', 'the', 'at', 're', 'for', 'covid-19', 'patients', 'with', 'negative', 'pressure', 'ventilation', 'uni', 'directional', 'l', 'amin', 'ar', 'flow', 'as', 'well', 'as', 'the', 'use', 'of', 'smoke', 'ev', 'ac', 'u', 'ation', 'systems', 'during', 'surgery', 'are', 'taken', 'into', 'account', '8', '[SEP]', 'such', 'an', 'elec', 'tiveness', 'of', 'surgery', 'is', 'concerning', 'as', 'it', 'might', 'cause', 'a', 'worldwide', 'healthcare', 'cat', 'astro', 'phe', 'in', 'the', 'post', '-', 'pandemic', 'era', '5', '[SEP]', 'more', 'efforts', 'should', 'be', 'taken', 'to', 'expand', 'the', 'amount', 'and', 'types', 'of', 'surgical', 'procedures', 'performed', '[SEP]', 'due', 'to', 'the', 'early', 'preventive', 'and', 'corre', 'ctive', 'actions', 'we', 'have', 'already', 'reached', 'the', 'plate', 'au', 'in', 'new', 'cases', 'curve', 'counting', 'for', 'up', 'to', '8', '98', '4', 'cases', 'identified', 'at', 'the', 'time', 'of', 'writing', 'this', 'paper', 'june', '7', 'th', '2020', 'with', 'a', 'mortality', 'rate', 'of', '150', '75', '[SEP]', 'these', 'statistical', 'outcomes', 'are', 'resulted', 'by', 'a', '68', '-', 'day', 'lockdown', 'admission', 'regime', 'and', 'healthcare', 're', 'arrangement', '[SEP]', 'thus', 'the', 'multi', 'step', 'restriction', 'lif', 'ting', 'has', 'already', 'started', 'to', 'consistently', 'recover', 'in', 'both', 'social', 'and', 'economic', 'aspects', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Usage of BPE tokenizer\n",
    "\n",
    "tokenizer = BPETokenizer(from_file=True, tokenizer_path=\"BPETokenizer15000.json\")\n",
    "# tokenizer = Tokenizer.from_file(\"BPETokenizerPro10000.json\")\n",
    "\n",
    "print(tokenizer.vocab_size)\n",
    "\n",
    "print(tokenizer.id_to_token(9999))\n",
    "print(tokenizer.token_to_id(\"college\"))\n",
    "\n",
    "output = tokenizer.encode(\"How are you? I'm fine, thank you, and you? I'm fine, too.\")\n",
    "print(output)\n",
    "print(tokenizer.decode(output[\"ids\"]))\n",
    "\n",
    "print(tokenizer.encode(\"diseases\")[\"tokens\"])\n",
    "print(tokenizer.encode(\"symptoms\")[\"tokens\"])\n",
    "print(tokenizer.encode(\"chemicals\")[\"tokens\"])\n",
    "print(tokenizer.encode(\"medications\")[\"tokens\"])\n",
    "print(tokenizer.encode(\"phenotypes\")[\"tokens\"])\n",
    "print(tokenizer.encode(\"genotypes\")[\"tokens\"])\n",
    "print(tokenizer.encode(\"COVID-19\")[\"tokens\"])\n",
    "print(tokenizer.encode(\"I have 30,000 apples.\")[\"tokens\"])\n",
    "\n",
    "tokenizer.add_tokens([\"phenotypes\", \"genotypes\", \"COVID-19\", \"30000\"])\n",
    "print(tokenizer.vocab_size)\n",
    "\n",
    "print(tokenizer.encode(\"phenotypes\")[\"tokens\"])\n",
    "print(tokenizer.encode(\"genotypes\")[\"tokens\"])\n",
    "print(tokenizer.encode(\"COVID-19\")[\"tokens\"])\n",
    "print(tokenizer.encode(\"I have 30,000 apples.\")[\"tokens\"])\n",
    "print(tokenizer.encode(\"The main symptoms of COVID-19 are fever and cough.\")[\"tokens\"])\n",
    "\n",
    "print(tokenizer.encode(full_texts[0])[\"tokens\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 3 (30%): Build Word Representations\n",
    "\n",
    "Build word representations for each extracted word. If the hardware resource is limited, you may limit the vocabulary size up to 10k words/tokens (or even smaller) and the dimension of representations up to 256.\n",
    "\n",
    "The following tracks can be developed independently. You are encouraged to divide the workload to each team member.\n",
    "\n",
    "### Track 3.1 (15%): Use N-gram Language Modeling\n",
    "\n",
    "N-gram Language Modeling is to predict a target word by using `n` words from previous context. Specifically,\n",
    "\n",
    "$P(w_i | w_{i-1}, w_{i-2}, ..., w_{i-n+1})$\n",
    "\n",
    "For example, given a sentence, `\"the main symptoms of COVID-19 are fever and cough\"`, if $n=7$, we use previous context `[\"the\", \"main\", \"symptoms\", \"of\", \"COVID-19\", \"are\"]` to predict the next word `\"fever\"`.\n",
    "\n",
    "More to read: https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "\n",
    "Recommended outputs:\n",
    "\n",
    "- A fixed vector for each word/token.\n",
    "\n",
    "### Track 3.2 (15%): Use Skip-gram with Negative Sampling\n",
    "\n",
    "In skip-gram, we use a central word to predict its context. Specifically,\n",
    "\n",
    "$P(w_{c-m}, ... w_{c-1}, w_{c+1}, ..., w_{c+m} | w_c)$\n",
    "\n",
    "As the learning objective of skip-gram is computational inefficient (summation of entire vocabulary $|V|$), negative sampling is commonly applied to accelerate the training.\n",
    "\n",
    "In negative sampling, we randomly select one word from the context as a positive sample, and randomly select $K$ words from the vocabulary as negative samples. As a result, the learning objective is updated to\n",
    "\n",
    "$L = -\\log\\sigma(u^T_{t} v_c) - \\sum_{k=1}^K\\log\\sigma(-u^T_k v_c)$, where $u_t$ is the vector embedding of positive sample from context, $u_k$ are the vector embeddings of negative samples, $v_c$ is the vector embedding of the central word, $\\sigma$ refers to the sigmoid function.\n",
    "\n",
    "More to read http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf section 4.3 and 4.4\n",
    "\n",
    "Recommended outputs:\n",
    "\n",
    "- A fixed vector for each word/token.\n",
    "\n",
    "### Track 3.3 (Bonus +5%): Use Contextualised Word Representation by Masked Language Model (MLM)\n",
    "\n",
    "BERT introduces a new language model for pre-training named Masked Language Model (MLM). The advantage of MLM is that the word representations by MLM will be contextualised.\n",
    "\n",
    "For example, \"stick\" may have different meanings in different context. By N-gram language modeling and word2vec (skip-gram, CBOW), the word representation of \"stick\" is fixed regardless of its context. However, MLM will learn the representation of \"stick\" dynamatically based on context. In other words, \"stick\" will have different representations in different context by MLM.\n",
    "\n",
    "More to read: http://jalammar.github.io/illustrated-bert/ and https://arxiv.org/pdf/1810.04805.pdf\n",
    "\n",
    "Recommended outputs:\n",
    "\n",
    "- An algorithm that is able to generate contextualised representation in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# TODO: add your solution\n",
    "\n",
    "###################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 (20%): Explore the Word Representations\n",
    "\n",
    "The following tracks can be finished independently. You are encouraged to divide workload to each team member.\n",
    "\n",
    "### Track 4.1 (5%): Visualise the word representations by t-SNE\n",
    "\n",
    "t-SNE is an algorithm to reduce dimentionality and commonly used to visualise high-dimension vectors. Use t-SNE to visualise the word representations. You may visualise up to 1000 words as t-SNE is highly computationally complex.\n",
    "\n",
    "More about t-SNE: https://lvdmaaten.github.io/tsne/\n",
    "\n",
    "Recommended output:\n",
    "\n",
    "- A diagram by t-SNE based on representations of up to 1000 words.\n",
    "\n",
    "### Track 4.2 (5%): Visualise the Word Representations of Biomedical Entities by t-SNE\n",
    "\n",
    "Instead of visualising the word representations of the entire vocabulary (or 1000 words that are selected at random), visualise the word representations of words which are biomedical entities. For example, fever, cough, diabetes etc. Based on the category of those biomedical entities, can you assign different colours to the entities and see if the entities from the same category can be clustered by t-SNE? For example, sinusitis and cough are both respirtory diseases so they should be assigned with the same colour and ideally their representations should be close to each other by t-SNE. Another example, Alzheimer and headache are neuralogical diseases which should be assigned by another colour.\n",
    "\n",
    "Examples of biomedial ontology: https://www.ebi.ac.uk/ols/ontologies/hp and https://en.wikipedia.org/wiki/International_Classification_of_Diseases\n",
    "\n",
    "Recommended output:\n",
    "\n",
    "- A diagram with colours by t-SNE based on representations of biomedical entities.\n",
    "\n",
    "### Track 4.3 (5%): Co-occurrence\n",
    "\n",
    "- What are the biomedical entities which frequently co-occur with COVID-19 (or coronavirus)?\n",
    "\n",
    "Recommended outputs:\n",
    "\n",
    "- A sorted list of biomedical entities and description on how the entities are selected and sorted.\n",
    "\n",
    "### Track 4.4 (5%): Semantic Similarity\n",
    "\n",
    "- What are the biomedical entities which have closest semantic similarity COVID-19 (or coronavirus) based on word representations?\n",
    "\n",
    "Recommended outputs:\n",
    "\n",
    "- A sorted list of biomedical entities and description on how the entities are selected and sorted.\n",
    "\n",
    "### Open Question (Optional): What else can you discover?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# TODO: add your solution\n",
    "\n",
    "###################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 (Bonus +10%): Open Challenge: Mining Biomedical Knowledge\n",
    "\n",
    "A fundamental task in clinical/biomedical natural language processing is to extract intelligence from biomedical text corpus automatically and efficiently. More specifically, the intelligence may include biomedical entities mentioned in text, relations between biomedical entities, clinical features of patients, progression of diseases, all of which can be used to predict, understand and improve patients' outcomes. \n",
    "\n",
    "This open challenge is to build a biomedical knowledge graph based on the CORD-19 dataset and mine useful information from it. We recommend the following steps but you are also encouraged to develop your solution from scratch.\n",
    "\n",
    "### Extract Biomedical Entities from Text\n",
    "\n",
    "Extract biomedical entities (such as fever, cough, headache, lung cancer, heart attack) from text. Note that:\n",
    "\n",
    "- The biomedical entities may consist of multiple words. For example, heart attack, multiple myeloma etc.\n",
    "- The biomedical entities may be written in synoynms. For example, low blood pressure for hypotension.\n",
    "- The biomedical entities may be written in different forms. For example, smoking, smokes, smoked.\n",
    "\n",
    "### Extract Relations between Biomedical Entities\n",
    "\n",
    "Extract relations between biomedical entities based on their appearance in text. You may define a relation between biomedical entities by one or more of the following criteria:\n",
    "\n",
    "- The biomedical entities frequentely co-occuer together.\n",
    "- The biomedical entities have similar word representations.\n",
    "- The biomedical entities have clear relations based on textual narratives. For example, \"The most common symptoms for COVID-19 are fever and cough\" so we know there are relations between \"COVID-19\", \"fever\" and \"cough\".\n",
    "\n",
    "### Build a Biomedical Knowledge Graph of COVID-19\n",
    "\n",
    "Build a knoweledge graph based on the results from track 5.1 and 5.2 and visualise it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# TODO: add your solution\n",
    "\n",
    "###################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ee6028153bae1c310a6c17909bbeb01b64dc45f74f1d212e5f04026a0eaac9d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
